# Default LoRA Configuration

# LoRA hyperparameters
r: 16                          # LoRA rank (test 8, 16, 32, 64)
lora_alpha: 32                 # Scale factor (typically 2*r)
lora_dropout: 0.1              # Dropout for regularization

# Target modules for adaptation
target_modules:
  - "q_proj"                   # Query projection
  - "v_proj"                   # Value projection
  # Optional additional modules:
  # - "k_proj"                 # Key projection
  # - "out_proj"               # Output projection
  # - "fc1"                    # FFN first layer
  # - "fc2"                    # FFN second layer

# LoRA configuration
bias: "none"                   # Don't train bias terms
modules_to_save: null          # Additional modules to train (e.g., layer norm)

# Rank configurations for experiments
rank_configs:
  small:
    r: 8
    lora_alpha: 16
  medium:
    r: 16
    lora_alpha: 32
  large:
    r: 32
    lora_alpha: 64
  xlarge:
    r: 64
    lora_alpha: 128
